# Information Theory

为了更好地对机器学习中的某些概念作出解释，需要对一些基础的信息论知识作出补充。信息论，即研究信息量的科学，其首次对信息这一抽象概念作出定义，导出定理，成了一门定量的、可计算的科学。

## Intuition

信息是什么？面对未知，我们可以假设其存在任何可能。而信息，则能够抹除部分可能，削减部分可能同时强化部分可能，使我们对未知的判断有更深刻的认识。

落到实处，为了使用数学的方法对未知、信息这些概念进行解析与应用，我们需要引入数学工具：

1. 随机变量：我们使用随机变量来表征未知事件
2. bit符号：我们使用bit符号来表征信息

由此，我们可以对随机变量某一取值的未知程度、随机变量未知程度的期望作出数学化的解释。

## Self-Information

Self-Information，也即自信息，表征随机变量某一取值所具有的信息量。

具象化来说，针对某一随机变量做分析——明天下雨的这一随机事件：

1. 在没有任何已知条件的情况下，明天下雨与不下雨我们只能认为55开，我们不知道确切结果，这便是未知。给定“明天下雨”这一信息，该消息具备的信息量也就消除了**下雨与否**这一随机变量的未知性。
2. 而如果知道天气预报，那么我们出于对天气预报的信任，那么明天下雨这件事可能就是82开了，其中的未知少一些，“明天下雨”带来的信息就没那么大了；而此时，“明天不下雨”，这一与预期相对不同的信号，带来的信息量就较大了。

结果来看，我们需要一个数学概念来定量化**信息量**，亦即一个随机变量的给定**结果**，将不确定变为确定这个过程中获得的**信息量**。其应该满足如下性质：

1. 独立变量的**结果**之间的**信息量**可加和
2. 信息量非负
3. 与随机变量这一**结果**的概率有关，且与成负相关

我们知道，两个独立随机变量，各自的特定情况一起发生的概率为：
$$p(X,Y)=p(X)\times p(Y)$$ 那么，就信息量而言，我们定义为h(X),则应该有：
$$I(X,Y)=I(X)+I(Y)$$ 于是，前人发现了一个能够具有以上特征的函数：
$$ I(X) = -\log p(X)$$ 此即**自信息**，表征一个结果所具备的**信息量**。

## Entropy

就单个结果带有的信息量，我们已做出了讨论，但，对于整个随机变量来说呢？显然，我们需要一个值来考量任意一给定结果所具备的信息量（自信息），其应该具备以下性质：

1. 需要综合随机变量各个取值的自信息
2. 需要考虑到随机变量取值占的比重

香农在其论文中给出信息熵（Entropy）的定义：
$$H(X)=\mathrm{E}_{p}[-\log q]=-\sum_{i=1}^Np_i(X)\log p_i(X)$$ 信息熵，即**自信息**相对随机变量的数学期望。较好地考察了随机变量这一体系下蕴含的未知（给定结果能够消除的未知，结果的自信息）。

## Cross Entropy

熵这一概念有许多应用，交叉熵亦是其应用之一。在此之前，对于随机变量分布之间的差别，缺乏一个合适的度量，交叉熵与我们后续讨论的KL散度就是一个对比分布间差别的有力工具。

目的是比较分布之间的差别，而熵则是分布的一个特征，看似各自求熵，计算差值即可。然而，不同的分布，将导致求期望时实际上影响了各个可能的自信息求和时的表现。为了排除这种干扰，我们需要在同一测度（分布）下考量熵。

在此，我们先给出交叉熵的定义：
$$H(p, q)=\mathrm{E}_{p}[-\log q]=H(p)+D_{\mathrm{KL}}(p \| q)$$ 交叉熵涉及两个关于随机变量的分布p,q。从其定义中可以看出，其考察了分布q下的自信息按照分布p下的权重进行加权得到的结果。

有了交叉熵，跨分布比较就成了可能。

## KL Divergence

既然已经有方法跨分布计算熵值了，我们便可以通过作差度量分布间的差别了。

KL散度的定义：
$$D_{\mathrm{KL}}(P \| Q)=\sum_{i} P(i) \ln \frac{P(i)}{Q(i)}$$

结合散度定义与交叉熵定义，我们不难发现:$$D_{\mathrm{KL}}(p \| q)=H(p,q)-H(p)$$ 即，q在p下的交叉熵减去p的熵，这就是p分布下q分布与p的差别。

还应当注意到，KL散度不是**距离**，因为KL散度是非对称的：$$D_{KL}(P||Q)\neq D_{KL}(Q||P) $$

## Origin —— Symbol Respective

至此，已经从较为现代化的角度对信息论，及其中概念的意义。但是，信息论，究其根本，来源于香农对信号在信道中传播的研究，为了更好地对前文中较为晦涩的内容进行理解（内在化），我们还是得从信号/符号的角度来对信息论进行诠释。

同时，该函数还具有一个奇妙的性质，即h(X)表彰了